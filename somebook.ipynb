{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from __future__ import absolute_import, division, print_function\r\n",
                "import os\r\n",
                "\r\n",
                "from transformers import BartTokenizer, BertTokenizer, T5Tokenizer, BertGenerationConfig, BertConfig\r\n",
                "import wandb\r\n",
                "from eval import Evaluator\r\n",
                "from training import train, set_seed\r\n",
                "from cli import parse_args\r\n",
                "from data import get_dataset\r\n",
                "from modeling import OurModel\r\n",
                "import torch\r\n",
                "import git\r\n",
                "import shutil\r\n",
                "import logging\r\n",
                "from collections import OrderedDict\r\n",
                "import json\r\n",
                "import pickle\r\n",
                "\r\n",
                "logger = logging.getLogger(__name__)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def main():\r\n",
                "\r\n",
                "    args = parse_args()\r\n",
                "\r\n",
                "    transformers_logger = logging.getLogger(\"transformers\")\r\n",
                "    transformers_logger.setLevel(logging.ERROR)\r\n",
                "\r\n",
                "    # Setup logging\r\n",
                "    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\r\n",
                "                        datefmt='%m/%d/%Y %H:%M:%S',\r\n",
                "                        level=logging.INFO)\r\n",
                "\r\n",
                "    # Setup CUDA, GPU & distributed training\r\n",
                "    device = torch.device(f\"cuda:{args.gpu_id}\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\r\n",
                "    args.n_gpu = torch.cuda.device_count()\r\n",
                "\r\n",
                "    args.device = device\r\n",
                "\r\n",
                "\r\n",
                "    # Set seed\r\n",
                "    set_seed(12)\r\n",
                "\r\n",
                "    if args.model_type == 'bert_generation':\r\n",
                "        tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\r\n",
                "        model = OurModel(args)\r\n",
                "    elif args.model_type == 'bart' or args.model_type == 'bart-raw':\r\n",
                "        num_extra_tokens = 101\r\n",
                "        extra_tokens = [f'<extra_token_{i}>' for i in range(num_extra_tokens)]\r\n",
                "        tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\r\n",
                "        tokenizer.add_tokens(extra_tokens)\r\n",
                "        model = OurModel(args, tokenizer)\r\n",
                "    elif args.model_type == 't5' or args.model_type == 't5-raw':\r\n",
                "        tokenizer = T5Tokenizer.from_pretrained('t5-small')\r\n",
                "        model = OurModel(args)\r\n",
                "    else:\r\n",
                "        raise ValueError(\"model_type should be one of [None,'bart','t5']\")\r\n",
                "\r\n",
                "    if args.cont:\r\n",
                "        model.from_pretrained(args.cont)\r\n",
                "\r\n",
                "\r\n",
                "    # sd = model.model.state_dict()\r\n",
                "    # with open('state_dict.pkl', 'wb') as f:\r\n",
                "    #     pickle.dump(sd, f)\r\n",
                "    # exit()\r\n",
                "\r\n",
                "    model.to(args.device)\r\n",
                "\r\n",
                "    logger.info(\"Evaluation parameters %s\", args)\r\n",
                "\r\n",
                "    evaluator = Evaluator(args, tokenizer)\r\n",
                "\r\n",
                "    sweep = {}\r\n",
                "    beam_sizes = [6,7,8,9,10,11,12,13,14,15]\r\n",
                "    for bs in beam_sizes:\r\n",
                "        # Evaluation\r\n",
                "        result = evaluator.evaluate_beam(\r\n",
                "            model, prefix=f\"{args.model_type}_{args.cont.split('/')[-1]}_dev\", beam_size=bs)\r\n",
                "        sweep[bs] = result\r\n",
                "\r\n",
                "    with open(os.path.join(args.output_dir, 'dev_beam_sweep.json'), 'w') as f:\r\n",
                "        f.write(json.dumps(OrderedDict(sweep)))\r\n",
                "\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "main()"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}